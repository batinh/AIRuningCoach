import os
import logging
import google.generativeai as genai
from tools.notify_tools import send_telegram_msg

# --- GLOBAL MEMORY (B·ªô nh·ªõ ng·∫Øn h·∫°n - RAM) ---
# C·∫•u tr√∫c: { "chat_id": [history_object, ...] }
CHAT_HISTORY = {}
MAX_HISTORY_LEN = 20  # Ch·ªâ nh·ªõ 20 c√¢u g·∫ßn nh·∫•t ƒë·ªÉ ti·∫øt ki·ªám Token

# Configure logging
logger = logging.getLogger(__name__)
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

# --- 1. WORKFLOW PH√ÇN T√çCH B√ÄI CH·∫†Y (Gi·ªØ nguy√™n logic c≈©) ---
def analyze_run_with_gemini(activity_id: str, activity_name: str, csv_data: str, config: dict):
    logger.info(f"[COACH AGENT] Analyzing run: {activity_name} (ID: {activity_id})")

    # 1. Setup Context
    system_instruction = config.get("system_instruction", "You are Coach Dyno.")
    user_profile = config.get("user_profile", "")
    
    # Gh√©p Profile v√†o System Instruction ƒë·ªÉ Bot hi·ªÉu s√¢u h∆°n
    full_instruction = f"{system_instruction}\n\n[USER PROFILE DATA]\n{user_profile}"
    
    analysis_requirements = config.get("analysis_requirements", "Analyze HR and Power.")
    output_format = config.get("output_format", "Output in Vietnamese.")
    current_model_name = config.get("model_name", "models/gemini-2.0-flash")

    # 2. Kh·ªüi t·∫°o Model
    try:
        model = genai.GenerativeModel(
            model_name=current_model_name,
            system_instruction=full_instruction
        )
    except Exception as e:
        logger.error(f"Error initializing model {current_model_name}: {e}")
        return None

    # 3. T·∫°o Prompt
    prompt = f"""
    [TASK CONTEXT]
    Activity: {activity_name}
    
    [ANALYSIS REQUIREMENTS]
    {analysis_requirements}
    
    [OUTPUT FORMAT]
    {output_format}
    
    [RAW CSV DATA]
    {csv_data}
    """
    
    # Debug
    if config.get("debug_mode"):
        logger.info(f"[SYSTEM] Analyzing with Model: {current_model_name}")
        # Log prompt ·∫©n data
        log_prompt = prompt.replace(csv_data, f"\n[...CSV HIDDEN {len(csv_data)} bytes...]\n")
        logger.info(f"[PROMPT PREVIEW]\n{log_prompt}")

    try:
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        logger.error(f"[COACH AGENT] Gemini Error: {e}")
        return None

# --- 2. WORKFLOW CHAT TELEGRAM (N√ÇNG C·∫§P C√ì TR√ç NH·ªö) ---
def handle_telegram_chat(chat_id: str, text: str, config: dict):
    """
    X·ª≠ l√Ω chat v·ªõi b·ªô nh·ªõ ƒë·ªám (Contextual Memory).
    """
    debug_mode = config.get("debug_mode", False)
    
    # A. X·ª≠ l√Ω l·ªánh ƒë·∫∑c bi·ªát
    if text.strip().lower() in ["/clear", "/reset", "x√≥a nh·ªõ"]:
        if chat_id in CHAT_HISTORY:
            del CHAT_HISTORY[chat_id]
        send_telegram_msg(chat_id, "üßπ ƒê√£ x√≥a b·ªô nh·ªõ t·∫°m. Ch√∫ng ta b·∫Øt ƒë·∫ßu l·∫°i nh√©!")
        return

    # B. C·∫•u h√¨nh "B·ªô n√£o"
    current_model_name = config.get("model_name", "models/gemini-2.0-flash")
    system_instruction = config.get("system_instruction", "You are Coach Dyno.")
    user_profile = config.get("user_profile", "")

    # C. Gh√©p "Nh√¢n c√°ch" + "Th√¥ng tin User" v√†o System Prompt
    # (ƒê√¢y l√† b√≠ quy·∫øt ƒë·ªÉ start_chat v·∫´n nh·ªõ b·∫°n l√† ai)
    full_persona = f"""
    {system_instruction}
    
    [USER PROFILE & CONTEXT]
    {user_profile}
    
    [INSTRUCTION]
    - You are chatting directly with the user via Telegram.
    - Keep responses concise, helpful, and friendly.
    - Remember previous context in this conversation.
    """

    try:
        model = genai.GenerativeModel(
            model_name=current_model_name,
            system_instruction=full_persona
        )
    except Exception as e:
        logger.error(f"[TELEGRAM] Model Error: {e}")
        send_telegram_msg(chat_id, f"‚ö†Ô∏è L·ªói model {current_model_name}. H√£y ƒë·ªïi model kh√°c tr√™n Web Admin.")
        return

    # D. Kh√¥i ph·ª•c l·ªãch s·ª≠ chat t·ª´ RAM
    # N·∫øu ch∆∞a c√≥ th√¨ t·∫°o list r·ªóng
    current_history = CHAT_HISTORY.get(chat_id, [])

    if debug_mode:
        logger.info(f"[TELEGRAM] Chatting with history ({len(current_history)} turns). Model: {current_model_name}")

    try:
        # E. B·∫ÆT ƒê·∫¶U CHAT V·ªöI L·ªäCH S·ª¨ C≈®
        chat_session = model.start_chat(history=current_history)
        
        # G·ª≠i tin nh·∫Øn m·ªõi
        response = chat_session.send_message(text)
        reply_text = response.text

        # F. C·∫≠p nh·∫≠t l·∫°i l·ªãch s·ª≠ v√†o RAM
        # Ch·ªâ gi·ªØ l·∫°i MAX_HISTORY_LEN tin m·ªõi nh·∫•t ƒë·ªÉ ti·∫øt ki·ªám
        updated_history = chat_session.history
        if len(updated_history) > MAX_HISTORY_LEN:
            updated_history = updated_history[-MAX_HISTORY_LEN:]
        
        CHAT_HISTORY[chat_id] = updated_history

        # G·ª≠i k·∫øt qu·∫£
        send_telegram_msg(chat_id, reply_text)
        
    except Exception as e:
        logger.error(f"[TELEGRAM] Chat Error: {e}")
        # N·∫øu l·ªói (do token qu√° d√†i ho·∫∑c model crash), th·ª≠ x√≥a nh·ªõ v√† chat l·∫°i 1 l·∫ßn
        if "400" in str(e) or "token" in str(e).lower():
            if chat_id in CHAT_HISTORY:
                del CHAT_HISTORY[chat_id]
                send_telegram_msg(chat_id, "‚ö†Ô∏è B·ªô nh·ªõ ƒë·∫ßy, t√¥i ƒë√£ t·ª± ƒë·ªông reset ƒë·ªÉ ti·∫øp t·ª•c cu·ªôc tr√≤ chuy·ªán.")
                # Th·ª≠ g·ªçi l·∫°i ƒë·ªá quy 1 l·∫ßn (c·∫©n th·∫≠n loop)
                # handle_telegram_chat(chat_id, text, config) 
        else:
            send_telegram_msg(chat_id, "‚ö†Ô∏è Coach Dyno ƒëang b·ªã 'chu·ªôt r√∫t' (L·ªói API). Th·ª≠ /clear xem sao!")